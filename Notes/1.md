- Decision Boundary

- Sensitivity - 90% of people test positive if they have cancer
- Specitivity - 90% of people test negative if they don't have cancer
$$Prior: P(C) = 0.01$$
$$Joint: P(C,Pos) = P(C).P(Pos|C) = 0.009, P(\neg C,Pos)= P(\neg C).P(Pos|\neg C) = 0.099$$
$$Normalie: P(C,Pos) + P(\neg C,Pos) = 0.009 + 0.099 = 0.108$$
$$Posterior: P(C|Pos) = 0.009/0.108 = 0.0833, P(\neg C|Pos) = 0.099/0.108 = 0.9167$$

Naive Bayes:
- Curved Decision Boudnary
Advantages - good implementation for classifying texts with a large number of features - each word in text interpreted as an independent feature
Disadvantages - can break when the data fed is significantly different from what is to be predicted
training time: 0.487 s
prediction time 0.049 s
score 0.9732650739476678

SVM:
Advantages - used when there is a clear margin of separation
Disadvantages - inefficient for large datasets as training time is cubic of data set size. May have some noise overlapping for data classification. Does not do well with a large number of features. Naive Bayes is better for text - faster and gives better performance.
- Separating line: from nearest point - maximum distance - linear decision boundary
- Correct classification first, maximum margin to nearest point second
- Parameters:
	- Kernels
	- Gamma: controls how far the affect of a single data point reaches - low value means far reach(more linear decision boundary), high value means close reach (close points have more affect in determining decision boundary - means more jagged decision boundary)
	- C: controls tradeoff between smooth decision boundary and correctly classifying training points - higher C means correct classification, lower C means smoother decision boundary
training time: 80.077 s
prediction time: 7.953 s
score: 0.9840728100113766
- A smaller training set - better speed, less accuracy:
	- training time: 0.046 s
	- prediction time: 0.445 s
	- score: 0.8845278725824801
	- by replacing linear kernel with rbf kernel:
		- training time: 0.053 s
		- prediction time: 0.661 s
		- score: 0.8953356086461889
- Optimised SVM with kernel='rbf' and C=1000.
training time: 79.729 s
prediction time: 11.18 s
score: 0.9960182025028441


Decision Trees:
- can ask multiple linear questions
- {'acc_min_samples_split_50': 0.912, 'acc_min_samples_split_2': 0.908}
- entropy is the opposite of purity - from 0 to 1.0
	$$Entropy = -\sum_i p_ilog_2(pi)$$
$$p_{class} = class \> elements/total \> elements$$
$$weighted \> average \> entropy = \sum(class \> elements/total \>elements)*entropy \> of \> children \> in \> a \> class $$
$$information \> gain = entropy(parent) - [weighted \> average]entropy(children)$$
- Bias - does not learn well from the data - oversimplified training. - **high error on training set.** - **Using few features in training.**
- Variance - prediction too dependent on training data - overfitting. - **high error on test set.**
- Advantages - good for graphing data and interpreting data.
- Disadvantages - prone to overfitting with data - need to be managed with parameters to control tree growth.
- With SelectPercentile(percentile=10):
training time: 15.549 s
prediction time: 0.015 s
score = 0.9778156996587031
features: 3785
- With SelectPercentile(percentile=1):

New Algorithms:
- Do some research
- Find sklearn documentation
- deploy it
- use it to make predictions
- evaluate it's accuracy

Naive Bayes, SVM, Decision Tree, AdaBoost, Random Forest, KNN

AdaBoost:
DTC = tree.DecisionTreeClassifier(min_samples_split=50)
clf = AdaBoostClassifier(estimator=DTC,n_estimators=100,random_state=0)
- accuracy = 9.28, with good prediction and training time

RandomForestClassifier:
clf = RandomForestClassifier(max_depth=2,random_state=0)
- accuracy = 9.2

KNeighbours:
clf = KNeighborsClassifier(n_neighbors=3)
- accuracy = 9.36, decent prediction and training time
clf = KNeighborsClassifier(n_neighbors=4)
- accuracy = 9.4, decent prediction and training time

**A great machine learning researcher: has a lot of questions and wants to answer them, having this attitude makes you want to find the way to answer the question you have**

Enron Corpus